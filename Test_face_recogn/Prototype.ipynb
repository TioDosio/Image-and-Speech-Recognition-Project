{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tf-keras\n",
    "#!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from IPython.display import display\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "#from tensorflow_addons.losses import SigmoidFocalCrossEntropy\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import numpy as np\n",
    "sys.path.append('aux_scripts.py')\n",
    "\n",
    "i = \"3classes\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN creation and training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clear a directory\n",
    "def clear_directory(directory):\n",
    "    if os.path.exists(directory):\n",
    "        shutil.rmtree(directory)  # Remove the directory and its contents\n",
    "    os.makedirs(directory, exist_ok=True)  # Recreate an empty directory\n",
    "\n",
    "# Function to copy files to target directories\n",
    "def copy_files(file_list, target_dir):\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    for file in file_list:\n",
    "        shutil.copy(file, target_dir)\n",
    "\n",
    "# Function to split dataset\n",
    "def split_dataset(source_dir, test_size=0.2):\n",
    "    files = os.listdir(source_dir)\n",
    "    files = [os.path.join(source_dir, f) for f in files]\n",
    "    train_files, temp_files = train_test_split(files, test_size=test_size, random_state=42)\n",
    "    val_files, test_files = train_test_split(temp_files, test_size=0.5, random_state=42)\n",
    "    return train_files, val_files, test_files\n",
    "\n",
    "# Split the dataset\n",
    "def split_and_copy(train_dir, val_dir, test_dir):\n",
    "\n",
    "    # Clear the train, val, and test directories before splitting\n",
    "    clear_directory(train_dir)\n",
    "    clear_directory(val_dir)\n",
    "    clear_directory(test_dir)\n",
    "\n",
    "    mask_train_files, mask_val_files, mask_test_files = split_dataset(mask_dir)\n",
    "    no_mask_train_files, no_mask_val_files, no_mask_test_files = split_dataset(no_mask_dir)\n",
    "    wrong_mask_train_files, wrong_mask_val_files, wrong_mask_test_files = split_dataset(wrong_mask_dir)\n",
    "\n",
    "    # Copy files to train/val/test directories\n",
    "    copy_files(mask_train_files, os.path.join(train_dir, 'mask'))\n",
    "    copy_files(mask_val_files, os.path.join(val_dir, 'mask'))\n",
    "    copy_files(mask_test_files, os.path.join(test_dir, 'mask'))\n",
    "\n",
    "    copy_files(no_mask_train_files, os.path.join(train_dir, 'no_mask'))\n",
    "    copy_files(no_mask_val_files, os.path.join(val_dir, 'no_mask'))\n",
    "    copy_files(no_mask_test_files, os.path.join(test_dir, 'no_mask'))\n",
    "    \n",
    "    copy_files(wrong_mask_train_files, os.path.join(train_dir, 'wrong_mask'))\n",
    "    copy_files(wrong_mask_val_files, os.path.join(val_dir, 'wrong_mask'))\n",
    "    copy_files(wrong_mask_test_files, os.path.join(test_dir, 'wrong_mask'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to directories\n",
    "dataset_dir = '../dataset'\n",
    "train_dir = '../dataset_split/train'\n",
    "val_dir = '../dataset_split/val'\n",
    "test_dir = '../dataset_split/test'\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "mask_dir = os.path.join(dataset_dir, 'mask')\n",
    "no_mask_dir = os.path.join(dataset_dir, 'no_mask')\n",
    "wrong_mask_dir = os.path.join(dataset_dir, 'wrong_mask')\n",
    "\n",
    "split_and_copy(train_dir, val_dir, test_dir)\n",
    "\n",
    "# Data augmentation and generators\n",
    "train_datagen = ImageDataGenerator(rescale=1.0/255.0,\n",
    "                                   rotation_range=30,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True)\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "test_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "\n",
    "image_size = 299\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(train_dir, target_size=(image_size, image_size), batch_size=32, class_mode='categorical')\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(val_dir, target_size=(image_size, image_size), batch_size=32, class_mode='categorical')\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(test_dir, target_size=(image_size, image_size), batch_size=32, class_mode='categorical', shuffle=False)\n",
    "\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(image_size, image_size, 3))\n",
    "base_model.trainable = False  # Freeze the base model\n",
    "\n",
    "# Add custom layers on top\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "epochs = 200 \n",
    "\n",
    "# Early stopping and learning rate reduction callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # Monitor validation loss\n",
    "    patience=10,         # Stop training if no improvement for 10 epochs\n",
    "    restore_best_weights=True  # Restore the weights of the best epoch\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',  # Reduce learning rate when validation loss plateaus\n",
    "    factor=0.2,          # Reduce learning rate by a factor of 5\n",
    "    patience=5,          # Wait 5 epochs before reducing\n",
    "    min_lr=1e-6          # Set a floor for the learning rate\n",
    ")\n",
    "\n",
    "# Train the model with callbacks\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=epochs,\n",
    "    validation_data=val_generator,\n",
    "    callbacks= [early_stopping, reduce_lr]\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(test_generator)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# Save the model\n",
    "model.save(f'mask_detector_model_{i}.h5')\n",
    "\n",
    "# Print training summary\n",
    "print(\"Model trained and saved as 'mask_detector_model.h5'\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(test_generator)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load the model\n",
    "model = tf.keras.models.load_model(f'mask_detector_model_{i}.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F1-Score and Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get true labels\n",
    "y_true = test_generator.classes  # True class labels from the test generator\n",
    "\n",
    "# Get predicted labels\n",
    "y_pred_probs = model.predict(test_generator)  # Predict probabilities\n",
    "y_pred = np.argmax(y_pred_probs, axis=-1)     # Convert to class indices\n",
    "\n",
    "# Class labels (should match the order of your classes in test_generator.class_indices)\n",
    "class_labels = list(test_generator.class_indices.keys())\n",
    "\n",
    "# Classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=class_labels))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder containing images to test\n",
    "test_images_folder = '../dataset/images/cropped'  # Update with your folder path\n",
    "\n",
    "class_labels = ['Mask', 'No Mask', 'Wrong Mask']  # Update class labels accordingly\n",
    "\n",
    "for image_name in os.listdir(test_images_folder):\n",
    "    image_path = os.path.join(test_images_folder, image_name)\n",
    "    # Load and preprocess the image\n",
    "    img = load_img(image_path, target_size=(image_size, image_size))  # Resize to match input size\n",
    "    img_array = img_to_array(img) / 255.0  # Normalize pixel values\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = model.predict(img_array)\n",
    "    predicted_class = np.argmax(prediction)  # Get class index with highest probability\n",
    "    label = class_labels[predicted_class]  # Map index to class label\n",
    "    \n",
    "    display(img)\n",
    "    print(f\"Image: {image_name} - Prediction: {label} (Probabilities: {prediction[0]})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
