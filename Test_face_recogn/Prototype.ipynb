{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install deepface\n",
    "#!pip install tf-keras\n",
    "#!pip install scikit-learn\n",
    "#!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepface import DeepFace\n",
    "from PIL import Image, ImageDraw\n",
    "from IPython.display import display\n",
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "i=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the folder and image name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: race: 100%|██████████| 4/4 [00:00<00:00,  5.25it/s]  \n",
      "Action: race: 100%|██████████| 4/4 [00:00<00:00,  7.02it/s]  \n",
      "Action: race: 100%|██████████| 4/4 [00:00<00:00,  7.72it/s]  \n",
      "Action: race: 100%|██████████| 4/4 [00:00<00:00,  7.45it/s]  \n",
      "Action: race: 100%|██████████| 4/4 [00:00<00:00,  7.67it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cropped face saved as ../dataset/images/cropped/mix_mask_3.jpg_face_1.png\n",
      "Cropped face saved as ../dataset/images/cropped/mix_mask_3.jpg_face_2.png\n",
      "Cropped face saved as ../dataset/images/cropped/mix_mask_3.jpg_face_3.png\n",
      "Cropped face saved as ../dataset/images/cropped/mix_mask_3.jpg_face_4.png\n",
      "Cropped face saved as ../dataset/images/cropped/mix_mask_3.jpg_face_5.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: race: 100%|██████████| 4/4 [00:00<00:00,  6.07it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cropped face saved as ../dataset/images/cropped/w_mask_1.png_face_1.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: race: 100%|██████████| 4/4 [00:00<00:00,  7.46it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cropped face saved as ../dataset/images/cropped/test.jpg_face_1.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: race: 100%|██████████| 4/4 [00:00<00:00,  6.69it/s]  \n",
      "Action: race: 100%|██████████| 4/4 [00:00<00:00,  7.68it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cropped face saved as ../dataset/images/cropped/mix_mask_2.png_face_1.png\n",
      "Cropped face saved as ../dataset/images/cropped/mix_mask_2.png_face_2.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: race: 100%|██████████| 4/4 [00:00<00:00,  7.44it/s]  \n",
      "Action: race: 100%|██████████| 4/4 [00:00<00:00,  7.53it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cropped face saved as ../dataset/images/cropped/test3.png_face_1.png\n",
      "Cropped face saved as ../dataset/images/cropped/test3.png_face_2.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: race: 100%|██████████| 4/4 [00:00<00:00,  7.48it/s]  \n",
      "Action: race: 100%|██████████| 4/4 [00:00<00:00,  7.63it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cropped face saved as ../dataset/images/cropped/test2.png_face_1.png\n",
      "Cropped face saved as ../dataset/images/cropped/test2.png_face_2.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: race: 100%|██████████| 4/4 [00:00<00:00,  6.70it/s]  \n",
      "Action: race: 100%|██████████| 4/4 [00:00<00:00,  6.89it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cropped face saved as ../dataset/images/cropped/w_mask_2.png_face_1.png\n",
      "Cropped face saved as ../dataset/images/cropped/w_mask_2.png_face_2.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "folder_name = \"../dataset/images/original/\"\n",
    "modified_folder_name = \"../dataset/images/processed/\"\n",
    "cropped_folder_name = \"../dataset/images/cropped/\"\n",
    "\n",
    "# Create the processed and cropped folders if they don't exist\n",
    "os.makedirs(modified_folder_name, exist_ok=True)\n",
    "os.makedirs(cropped_folder_name, exist_ok=True)\n",
    "\n",
    "for image_name in os.listdir(folder_name):\n",
    "    # Only process if the file is an image (e.g., PNG, JPG)\n",
    "    if image_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "        # Load image\n",
    "        image_path = os.path.join(folder_name, image_name)\n",
    "        image = Image.open(image_path)\n",
    "\n",
    "        # Analyze the image\n",
    "        analysis = DeepFace.analyze(img_path=image_path, detector_backend='retinaface', enforce_detection=False)\n",
    "        \n",
    "        # Draw bounding boxes for each face detected\n",
    "        draw = ImageDraw.Draw(image)\n",
    "        for idx, item in enumerate(analysis):\n",
    "            region = item['region']\n",
    "            x, y, w, h = region['x'], region['y'], region['w'], region['h']\n",
    "\n",
    "            # Draw rectangle on the original image\n",
    "            draw.rectangle([(x, y), (x + w, y + h)], outline='green', width=4)\n",
    "\n",
    "            # Crop the detected face\n",
    "            cropped_face = image.crop((x, y, x + w, y + h))\n",
    "            cropped_face_path = os.path.join(cropped_folder_name, f\"{image_name}_face_{idx + 1}.png\")\n",
    "            cropped_face.save(cropped_face_path)\n",
    "            print(f\"Cropped face saved as {cropped_face_path}\")\n",
    "\n",
    "        # Save the image with bounding boxes\n",
    "        modified_image_path = os.path.join(modified_folder_name, \"MOD_\" + image_name)\n",
    "        image.save(modified_image_path)\n",
    "        \n",
    "        # Optionally, display the processed image\n",
    "        modified_image = Image.open(modified_image_path)\n",
    "        #display(modified_image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN creation and training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3274 images belonging to 2 classes.\n",
      "Found 818 images belonging to 2 classes.\n",
      "Epoch 1/100\n",
      "103/103 [==============================] - 68s 653ms/step - loss: 0.4880 - accuracy: 0.7816 - val_loss: 0.3784 - val_accuracy: 0.8496\n",
      "Epoch 2/100\n",
      "103/103 [==============================] - 66s 643ms/step - loss: 0.3575 - accuracy: 0.8592 - val_loss: 0.3100 - val_accuracy: 0.8729\n",
      "Epoch 3/100\n",
      "103/103 [==============================] - 76s 736ms/step - loss: 0.3338 - accuracy: 0.8638 - val_loss: 0.2632 - val_accuracy: 0.9071\n",
      "Epoch 4/100\n",
      "103/103 [==============================] - 72s 690ms/step - loss: 0.2996 - accuracy: 0.8867 - val_loss: 0.2412 - val_accuracy: 0.9034\n",
      "Epoch 5/100\n",
      "103/103 [==============================] - 69s 665ms/step - loss: 0.3012 - accuracy: 0.8833 - val_loss: 0.2007 - val_accuracy: 0.9279\n",
      "Epoch 6/100\n",
      "103/103 [==============================] - 69s 664ms/step - loss: 0.2932 - accuracy: 0.8891 - val_loss: 0.2188 - val_accuracy: 0.9205\n",
      "Epoch 7/100\n",
      "103/103 [==============================] - 70s 682ms/step - loss: 0.2836 - accuracy: 0.8958 - val_loss: 0.2143 - val_accuracy: 0.9181\n",
      "Epoch 8/100\n",
      "103/103 [==============================] - 71s 685ms/step - loss: 0.2729 - accuracy: 0.9001 - val_loss: 0.1971 - val_accuracy: 0.9181\n",
      "Epoch 9/100\n",
      "103/103 [==============================] - 70s 679ms/step - loss: 0.2687 - accuracy: 0.8934 - val_loss: 0.2929 - val_accuracy: 0.8912\n",
      "Epoch 10/100\n",
      "103/103 [==============================] - 66s 641ms/step - loss: 0.2586 - accuracy: 0.8971 - val_loss: 0.1986 - val_accuracy: 0.9218\n",
      "Epoch 11/100\n",
      "103/103 [==============================] - 67s 651ms/step - loss: 0.2555 - accuracy: 0.9029 - val_loss: 0.1861 - val_accuracy: 0.9340\n",
      "Epoch 12/100\n",
      "103/103 [==============================] - 71s 685ms/step - loss: 0.2539 - accuracy: 0.9013 - val_loss: 0.1650 - val_accuracy: 0.9377\n",
      "Epoch 13/100\n",
      "103/103 [==============================] - 65s 626ms/step - loss: 0.2341 - accuracy: 0.9081 - val_loss: 0.1450 - val_accuracy: 0.9450\n",
      "Epoch 14/100\n",
      "103/103 [==============================] - 65s 627ms/step - loss: 0.2421 - accuracy: 0.9105 - val_loss: 0.1391 - val_accuracy: 0.9401\n",
      "Epoch 15/100\n",
      "103/103 [==============================] - 65s 627ms/step - loss: 0.2312 - accuracy: 0.9114 - val_loss: 0.1366 - val_accuracy: 0.9413\n",
      "Epoch 16/100\n",
      "103/103 [==============================] - 65s 628ms/step - loss: 0.2278 - accuracy: 0.9120 - val_loss: 0.1252 - val_accuracy: 0.9511\n",
      "Epoch 17/100\n",
      "103/103 [==============================] - 64s 624ms/step - loss: 0.2221 - accuracy: 0.9151 - val_loss: 0.1174 - val_accuracy: 0.9499\n",
      "Epoch 18/100\n",
      "103/103 [==============================] - 64s 622ms/step - loss: 0.2570 - accuracy: 0.9032 - val_loss: 0.1604 - val_accuracy: 0.9279\n",
      "Epoch 19/100\n",
      "103/103 [==============================] - 53s 510ms/step - loss: 0.2202 - accuracy: 0.9175 - val_loss: 0.1122 - val_accuracy: 0.9560\n",
      "Epoch 20/100\n",
      "103/103 [==============================] - 53s 512ms/step - loss: 0.2080 - accuracy: 0.9227 - val_loss: 0.1017 - val_accuracy: 0.9572\n",
      "Epoch 21/100\n",
      "103/103 [==============================] - 53s 510ms/step - loss: 0.1923 - accuracy: 0.9297 - val_loss: 0.1408 - val_accuracy: 0.9438\n",
      "Epoch 22/100\n",
      "103/103 [==============================] - 53s 513ms/step - loss: 0.2077 - accuracy: 0.9209 - val_loss: 0.1116 - val_accuracy: 0.9548\n",
      "Epoch 23/100\n",
      "103/103 [==============================] - 53s 509ms/step - loss: 0.1904 - accuracy: 0.9297 - val_loss: 0.0993 - val_accuracy: 0.9584\n",
      "Epoch 24/100\n",
      "103/103 [==============================] - 53s 511ms/step - loss: 0.2011 - accuracy: 0.9221 - val_loss: 0.1005 - val_accuracy: 0.9621\n",
      "Epoch 25/100\n",
      "103/103 [==============================] - 53s 510ms/step - loss: 0.1724 - accuracy: 0.9362 - val_loss: 0.1006 - val_accuracy: 0.9584\n",
      "Epoch 26/100\n",
      "103/103 [==============================] - 53s 509ms/step - loss: 0.1802 - accuracy: 0.9334 - val_loss: 0.1381 - val_accuracy: 0.9499\n",
      "Epoch 27/100\n",
      "103/103 [==============================] - 53s 508ms/step - loss: 0.1886 - accuracy: 0.9267 - val_loss: 0.0819 - val_accuracy: 0.9621\n",
      "Epoch 28/100\n",
      "103/103 [==============================] - 52s 508ms/step - loss: 0.1785 - accuracy: 0.9346 - val_loss: 0.1167 - val_accuracy: 0.9572\n",
      "Epoch 29/100\n",
      "103/103 [==============================] - 52s 508ms/step - loss: 0.1825 - accuracy: 0.9346 - val_loss: 0.0852 - val_accuracy: 0.9694\n",
      "Epoch 30/100\n",
      "103/103 [==============================] - 58s 562ms/step - loss: 0.1731 - accuracy: 0.9337 - val_loss: 0.0822 - val_accuracy: 0.9658\n",
      "Epoch 31/100\n",
      "103/103 [==============================] - 67s 646ms/step - loss: 0.1618 - accuracy: 0.9435 - val_loss: 0.1072 - val_accuracy: 0.9548\n",
      "Epoch 32/100\n",
      "103/103 [==============================] - 65s 632ms/step - loss: 0.1524 - accuracy: 0.9404 - val_loss: 0.0955 - val_accuracy: 0.9609\n",
      "Epoch 33/100\n",
      "103/103 [==============================] - 62s 604ms/step - loss: 0.1381 - accuracy: 0.9453 - val_loss: 0.0918 - val_accuracy: 0.9719\n",
      "Epoch 34/100\n",
      "103/103 [==============================] - 62s 599ms/step - loss: 0.1331 - accuracy: 0.9508 - val_loss: 0.1031 - val_accuracy: 0.9621\n",
      "Epoch 35/100\n",
      "103/103 [==============================] - 62s 601ms/step - loss: 0.1509 - accuracy: 0.9389 - val_loss: 0.1439 - val_accuracy: 0.9425\n",
      "Epoch 36/100\n",
      "103/103 [==============================] - 62s 603ms/step - loss: 0.1397 - accuracy: 0.9465 - val_loss: 0.0934 - val_accuracy: 0.9609\n",
      "Epoch 37/100\n",
      "103/103 [==============================] - 62s 599ms/step - loss: 0.1499 - accuracy: 0.9423 - val_loss: 0.0840 - val_accuracy: 0.9633\n",
      "Epoch 38/100\n",
      "103/103 [==============================] - 62s 599ms/step - loss: 0.1550 - accuracy: 0.9453 - val_loss: 0.0953 - val_accuracy: 0.9609\n",
      "Epoch 39/100\n",
      "103/103 [==============================] - 62s 598ms/step - loss: 0.1406 - accuracy: 0.9462 - val_loss: 0.0628 - val_accuracy: 0.9780\n",
      "Epoch 40/100\n",
      "103/103 [==============================] - 62s 596ms/step - loss: 0.1238 - accuracy: 0.9545 - val_loss: 0.1265 - val_accuracy: 0.9474\n",
      "Epoch 41/100\n",
      "103/103 [==============================] - 62s 599ms/step - loss: 0.1249 - accuracy: 0.9527 - val_loss: 0.0666 - val_accuracy: 0.9719\n",
      "Epoch 42/100\n",
      "103/103 [==============================] - 62s 598ms/step - loss: 0.1233 - accuracy: 0.9514 - val_loss: 0.0601 - val_accuracy: 0.9780\n",
      "Epoch 43/100\n",
      "103/103 [==============================] - 62s 602ms/step - loss: 0.1220 - accuracy: 0.9569 - val_loss: 0.0633 - val_accuracy: 0.9768\n",
      "Epoch 44/100\n",
      "103/103 [==============================] - 64s 620ms/step - loss: 0.1293 - accuracy: 0.9545 - val_loss: 0.1010 - val_accuracy: 0.9633\n",
      "Epoch 45/100\n",
      "103/103 [==============================] - 63s 606ms/step - loss: 0.1347 - accuracy: 0.9530 - val_loss: 0.0628 - val_accuracy: 0.9719\n",
      "Epoch 46/100\n",
      "103/103 [==============================] - 62s 596ms/step - loss: 0.1334 - accuracy: 0.9548 - val_loss: 0.0616 - val_accuracy: 0.9743\n",
      "Epoch 47/100\n",
      "103/103 [==============================] - 62s 599ms/step - loss: 0.1192 - accuracy: 0.9545 - val_loss: 0.0759 - val_accuracy: 0.9707\n",
      "Epoch 48/100\n",
      "103/103 [==============================] - 62s 598ms/step - loss: 0.1243 - accuracy: 0.9545 - val_loss: 0.0662 - val_accuracy: 0.9670\n",
      "Epoch 49/100\n",
      "103/103 [==============================] - 62s 603ms/step - loss: 0.1179 - accuracy: 0.9594 - val_loss: 0.0819 - val_accuracy: 0.9670\n",
      "Epoch 50/100\n",
      "103/103 [==============================] - 62s 603ms/step - loss: 0.1056 - accuracy: 0.9624 - val_loss: 0.0693 - val_accuracy: 0.9694\n",
      "Epoch 51/100\n",
      "103/103 [==============================] - 62s 604ms/step - loss: 0.1064 - accuracy: 0.9600 - val_loss: 0.0544 - val_accuracy: 0.9743\n",
      "Epoch 52/100\n",
      "103/103 [==============================] - 62s 598ms/step - loss: 0.1034 - accuracy: 0.9615 - val_loss: 0.0537 - val_accuracy: 0.9804\n",
      "Epoch 53/100\n",
      "103/103 [==============================] - 62s 595ms/step - loss: 0.1067 - accuracy: 0.9618 - val_loss: 0.1000 - val_accuracy: 0.9597\n",
      "Epoch 54/100\n",
      "103/103 [==============================] - 62s 596ms/step - loss: 0.1381 - accuracy: 0.9511 - val_loss: 0.0629 - val_accuracy: 0.9743\n",
      "Epoch 55/100\n",
      "103/103 [==============================] - 62s 599ms/step - loss: 0.0987 - accuracy: 0.9640 - val_loss: 0.0564 - val_accuracy: 0.9792\n",
      "Epoch 56/100\n",
      "103/103 [==============================] - 63s 605ms/step - loss: 0.0996 - accuracy: 0.9600 - val_loss: 0.0518 - val_accuracy: 0.9731\n",
      "Epoch 57/100\n",
      "103/103 [==============================] - 62s 597ms/step - loss: 0.0974 - accuracy: 0.9621 - val_loss: 0.0617 - val_accuracy: 0.9768\n",
      "Epoch 58/100\n",
      "103/103 [==============================] - 62s 597ms/step - loss: 0.1172 - accuracy: 0.9551 - val_loss: 0.0659 - val_accuracy: 0.9780\n",
      "Epoch 59/100\n",
      "103/103 [==============================] - 62s 599ms/step - loss: 0.0919 - accuracy: 0.9670 - val_loss: 0.0947 - val_accuracy: 0.9707\n",
      "Epoch 60/100\n",
      "103/103 [==============================] - 63s 605ms/step - loss: 0.1018 - accuracy: 0.9655 - val_loss: 0.0771 - val_accuracy: 0.9792\n",
      "Epoch 61/100\n",
      "103/103 [==============================] - 62s 595ms/step - loss: 0.0941 - accuracy: 0.9655 - val_loss: 0.0496 - val_accuracy: 0.9804\n",
      "Epoch 62/100\n",
      "103/103 [==============================] - 62s 599ms/step - loss: 0.0925 - accuracy: 0.9685 - val_loss: 0.0536 - val_accuracy: 0.9804\n",
      "Epoch 63/100\n",
      "103/103 [==============================] - 62s 599ms/step - loss: 0.1006 - accuracy: 0.9670 - val_loss: 0.1003 - val_accuracy: 0.9719\n",
      "Epoch 64/100\n",
      "103/103 [==============================] - 62s 603ms/step - loss: 0.1026 - accuracy: 0.9640 - val_loss: 0.0488 - val_accuracy: 0.9804\n",
      "Epoch 65/100\n",
      "103/103 [==============================] - 62s 603ms/step - loss: 0.0984 - accuracy: 0.9652 - val_loss: 0.0601 - val_accuracy: 0.9707\n",
      "Epoch 66/100\n",
      "103/103 [==============================] - 63s 606ms/step - loss: 0.0896 - accuracy: 0.9701 - val_loss: 0.0640 - val_accuracy: 0.9731\n",
      "Epoch 67/100\n",
      "103/103 [==============================] - 63s 608ms/step - loss: 0.0711 - accuracy: 0.9731 - val_loss: 0.0646 - val_accuracy: 0.9743\n",
      "Epoch 68/100\n",
      "103/103 [==============================] - 63s 606ms/step - loss: 0.0804 - accuracy: 0.9728 - val_loss: 0.0371 - val_accuracy: 0.9853\n",
      "Epoch 69/100\n",
      "103/103 [==============================] - 62s 603ms/step - loss: 0.0857 - accuracy: 0.9673 - val_loss: 0.0775 - val_accuracy: 0.9694\n",
      "Epoch 70/100\n",
      "103/103 [==============================] - 62s 601ms/step - loss: 0.0850 - accuracy: 0.9692 - val_loss: 0.0482 - val_accuracy: 0.9792\n",
      "Epoch 71/100\n",
      "103/103 [==============================] - 62s 599ms/step - loss: 0.0919 - accuracy: 0.9682 - val_loss: 0.0689 - val_accuracy: 0.9780\n",
      "Epoch 72/100\n",
      "103/103 [==============================] - 62s 598ms/step - loss: 0.0864 - accuracy: 0.9716 - val_loss: 0.0471 - val_accuracy: 0.9804\n",
      "Epoch 73/100\n",
      "103/103 [==============================] - 62s 599ms/step - loss: 0.0826 - accuracy: 0.9688 - val_loss: 0.0399 - val_accuracy: 0.9829\n",
      "Epoch 74/100\n",
      "103/103 [==============================] - 62s 600ms/step - loss: 0.0860 - accuracy: 0.9719 - val_loss: 0.0431 - val_accuracy: 0.9792\n",
      "Epoch 75/100\n",
      "103/103 [==============================] - 62s 604ms/step - loss: 0.0885 - accuracy: 0.9692 - val_loss: 0.0352 - val_accuracy: 0.9878\n",
      "Epoch 76/100\n",
      "103/103 [==============================] - 62s 601ms/step - loss: 0.0843 - accuracy: 0.9704 - val_loss: 0.1033 - val_accuracy: 0.9682\n",
      "Epoch 77/100\n",
      "103/103 [==============================] - 62s 603ms/step - loss: 0.0719 - accuracy: 0.9734 - val_loss: 0.0636 - val_accuracy: 0.9682\n",
      "Epoch 78/100\n",
      "103/103 [==============================] - 61s 594ms/step - loss: 0.0757 - accuracy: 0.9743 - val_loss: 0.0647 - val_accuracy: 0.9768\n",
      "Epoch 79/100\n",
      "103/103 [==============================] - 62s 597ms/step - loss: 0.0828 - accuracy: 0.9698 - val_loss: 0.0909 - val_accuracy: 0.9707\n",
      "Epoch 80/100\n",
      "103/103 [==============================] - 62s 598ms/step - loss: 0.0903 - accuracy: 0.9673 - val_loss: 0.0398 - val_accuracy: 0.9853\n",
      "Epoch 81/100\n",
      "103/103 [==============================] - 62s 599ms/step - loss: 0.0795 - accuracy: 0.9698 - val_loss: 0.0571 - val_accuracy: 0.9756\n",
      "Epoch 82/100\n",
      "103/103 [==============================] - 73s 711ms/step - loss: 0.0777 - accuracy: 0.9734 - val_loss: 0.0413 - val_accuracy: 0.9829\n",
      "Epoch 83/100\n",
      "103/103 [==============================] - 64s 621ms/step - loss: 0.0790 - accuracy: 0.9707 - val_loss: 0.0335 - val_accuracy: 0.9890\n",
      "Epoch 84/100\n",
      "103/103 [==============================] - 72s 701ms/step - loss: 0.0647 - accuracy: 0.9786 - val_loss: 0.0316 - val_accuracy: 0.9841\n",
      "Epoch 85/100\n",
      "103/103 [==============================] - 73s 707ms/step - loss: 0.0652 - accuracy: 0.9756 - val_loss: 0.0738 - val_accuracy: 0.9743\n",
      "Epoch 86/100\n",
      "103/103 [==============================] - 71s 691ms/step - loss: 0.0795 - accuracy: 0.9734 - val_loss: 0.0418 - val_accuracy: 0.9829\n",
      "Epoch 87/100\n",
      "103/103 [==============================] - 72s 699ms/step - loss: 0.0846 - accuracy: 0.9679 - val_loss: 0.0373 - val_accuracy: 0.9878\n",
      "Epoch 88/100\n",
      "103/103 [==============================] - 72s 697ms/step - loss: 0.0713 - accuracy: 0.9740 - val_loss: 0.0453 - val_accuracy: 0.9817\n",
      "Epoch 89/100\n",
      "103/103 [==============================] - 72s 696ms/step - loss: 0.0613 - accuracy: 0.9783 - val_loss: 0.1052 - val_accuracy: 0.9780\n",
      "Epoch 90/100\n",
      "103/103 [==============================] - 72s 696ms/step - loss: 0.0891 - accuracy: 0.9716 - val_loss: 0.0503 - val_accuracy: 0.9792\n",
      "Epoch 91/100\n",
      "103/103 [==============================] - 72s 697ms/step - loss: 0.0784 - accuracy: 0.9743 - val_loss: 0.0593 - val_accuracy: 0.9817\n",
      "Epoch 92/100\n",
      "103/103 [==============================] - 67s 648ms/step - loss: 0.0813 - accuracy: 0.9722 - val_loss: 0.0457 - val_accuracy: 0.9804\n",
      "Epoch 93/100\n",
      "103/103 [==============================] - 63s 612ms/step - loss: 0.0611 - accuracy: 0.9774 - val_loss: 0.0885 - val_accuracy: 0.9780\n",
      "Epoch 94/100\n",
      "103/103 [==============================] - 65s 627ms/step - loss: 0.0630 - accuracy: 0.9774 - val_loss: 0.0407 - val_accuracy: 0.9890\n",
      "Epoch 95/100\n",
      "103/103 [==============================] - 64s 618ms/step - loss: 0.0657 - accuracy: 0.9768 - val_loss: 0.0357 - val_accuracy: 0.9914\n",
      "Epoch 96/100\n",
      "103/103 [==============================] - 61s 593ms/step - loss: 0.0658 - accuracy: 0.9771 - val_loss: 0.0351 - val_accuracy: 0.9841\n",
      "Epoch 97/100\n",
      "103/103 [==============================] - 61s 591ms/step - loss: 0.0733 - accuracy: 0.9725 - val_loss: 0.0364 - val_accuracy: 0.9853\n",
      "Epoch 98/100\n",
      "103/103 [==============================] - 61s 593ms/step - loss: 0.0694 - accuracy: 0.9753 - val_loss: 0.0519 - val_accuracy: 0.9792\n",
      "Epoch 99/100\n",
      "103/103 [==============================] - 61s 589ms/step - loss: 0.0698 - accuracy: 0.9740 - val_loss: 0.0289 - val_accuracy: 0.9902\n",
      "Epoch 100/100\n",
      "103/103 [==============================] - 61s 589ms/step - loss: 0.0666 - accuracy: 0.9808 - val_loss: 0.1489 - val_accuracy: 0.9560\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"int\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 94\u001b[0m\n\u001b[1;32m     91\u001b[0m i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# Save the model\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmask_detector_model\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mi\u001b[49m\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Print training summary\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel trained and saved as \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmask_detector_model.h5\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"int\") to str"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Paths to directories\n",
    "dataset_dir = '../dataset'\n",
    "train_dir = '../dataset_split/train'\n",
    "val_dir = '../dataset_split/val'\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "\n",
    "mask_dir = os.path.join(dataset_dir, 'mask')\n",
    "no_mask_dir = os.path.join(dataset_dir, 'no_mask')\n",
    "\n",
    "# Function to copy files to target directories\n",
    "def copy_files(file_list, target_dir):\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    for file in file_list:\n",
    "        shutil.copy(file, target_dir)\n",
    "\n",
    "# Split the dataset\n",
    "def split_and_copy():\n",
    "    mask_train_files, mask_val_files = split_dataset(mask_dir)\n",
    "    no_mask_train_files, no_mask_val_files = split_dataset(no_mask_dir)\n",
    "\n",
    "    # Copy files to train/val directories\n",
    "    copy_files(mask_train_files, os.path.join(train_dir, 'mask'))\n",
    "    copy_files(mask_val_files, os.path.join(val_dir, 'mask'))\n",
    "    copy_files(no_mask_train_files, os.path.join(train_dir, 'no_mask'))\n",
    "    copy_files(no_mask_val_files, os.path.join(val_dir, 'no_mask'))\n",
    "\n",
    "split_and_copy()\n",
    "\n",
    "# Data augmentation and generators\n",
    "train_datagen = ImageDataGenerator(rescale=1.0/255.0,\n",
    "                                   rotation_range=30,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True)\n",
    "val_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "# Build the CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "epochs = 100\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=epochs,\n",
    "    validation_data=val_generator\n",
    ")\n",
    "\n",
    "i+=1\n",
    "\n",
    "# Save the model\n",
    "model.save('mask_detector_model',i,'.h5')\n",
    "\n",
    "# Print training summary\n",
    "print(\"Model trained and saved as 'mask_detector_model.h5'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('mask_detector_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dataset/train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m validation_datagen \u001b[38;5;241m=\u001b[39m ImageDataGenerator(rescale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Load the training dataset from the directory\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m train_generator \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_datagen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflow_from_directory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Directory for the training images\u001b[39;49;00m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Resize the images\u001b[39;49;00m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbinary\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Binary classification (mask/no mask)\u001b[39;49;00m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Shuffle training images\u001b[39;49;00m\n\u001b[1;32m     21\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Load the validation dataset from the directory\u001b[39;00m\n\u001b[1;32m     24\u001b[0m validation_generator \u001b[38;5;241m=\u001b[39m validation_datagen\u001b[38;5;241m.\u001b[39mflow_from_directory(\n\u001b[1;32m     25\u001b[0m     val_dir,  \u001b[38;5;66;03m# Directory for the validation images\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     target_size\u001b[38;5;241m=\u001b[39mimage_size,  \u001b[38;5;66;03m# Resize the images\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# Do not shuffle validation images\u001b[39;00m\n\u001b[1;32m     30\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/Image-and-Speech-Recognition-Project/.venv/lib/python3.12/site-packages/tf_keras/src/preprocessing/image.py:1649\u001b[0m, in \u001b[0;36mImageDataGenerator.flow_from_directory\u001b[0;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[1;32m   1562\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflow_from_directory\u001b[39m(\n\u001b[1;32m   1563\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1564\u001b[0m     directory,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1578\u001b[0m     keep_aspect_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1579\u001b[0m ):\n\u001b[1;32m   1580\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Takes the path to a directory & generates batches of augmented data.\u001b[39;00m\n\u001b[1;32m   1581\u001b[0m \n\u001b[1;32m   1582\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1647\u001b[0m \u001b[38;5;124;03m        and `y` is a numpy array of corresponding labels.\u001b[39;00m\n\u001b[1;32m   1648\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1649\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDirectoryIterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1650\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1651\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1652\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1653\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolor_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1654\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_aspect_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_aspect_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1655\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclasses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1656\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1657\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1658\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1659\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1660\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1661\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_to_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_to_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1662\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_links\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_links\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m        \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Image-and-Speech-Recognition-Project/.venv/lib/python3.12/site-packages/tf_keras/src/preprocessing/image.py:563\u001b[0m, in \u001b[0;36mDirectoryIterator.__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio, dtype)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m classes:\n\u001b[1;32m    562\u001b[0m     classes \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m subdir \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[1;32m    564\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory, subdir)):\n\u001b[1;32m    565\u001b[0m             classes\u001b[38;5;241m.\u001b[39mappend(subdir)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dataset/train'"
     ]
    }
   ],
   "source": [
    "# Paths to the training and validation datasets\n",
    "train_dir = 'dataset/train'\n",
    "val_dir = 'dataset/val'\n",
    "\n",
    "# Image size (resize all images to the same size)\n",
    "image_size = (128, 128)\n",
    "\n",
    "# Rescale the pixel values for training set (without augmentation)\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Rescale the pixel values for validation set (no augmentation)\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Load the training dataset from the directory\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,  # Directory for the training images\n",
    "    target_size=image_size,  # Resize the images\n",
    "    batch_size=32,\n",
    "    class_mode='binary',  # Binary classification (mask/no mask)\n",
    "    shuffle=True  # Shuffle training images\n",
    ")\n",
    "\n",
    "# Load the validation dataset from the directory\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    val_dir,  # Directory for the validation images\n",
    "    target_size=image_size,  # Resize the images\n",
    "    batch_size=32,\n",
    "    class_mode='binary',  # Binary classification (mask/no mask)\n",
    "    shuffle=False  # Do not shuffle validation images\n",
    ")\n",
    "\n",
    "# Build the CNN model\n",
    "model = models.Sequential([\n",
    "    # Convolutional Layer 1\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Convolutional Layer 2\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Convolutional Layer 3\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Flatten the output to feed into fully connected layers\n",
    "    layers.Flatten(),\n",
    "    \n",
    "    # Dense Layer 1\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    \n",
    "    # Output Layer: Sigmoid activation for binary classification\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model with binary crossentropy loss (for binary classification) and Adam optimizer\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    "    epochs=10,  # Number of epochs to train the model\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // validation_generator.batch_size\n",
    ")\n",
    "\n",
    "i=1\n",
    "# Save the trained model\n",
    "model.save('face_mask_classifier_no_augmentatio_'+i+'.h5')\n",
    "i+=1\n",
    "# Plot accuracy and loss curves\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='train accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='validation accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='train loss')\n",
    "plt.plot(history.history['val_loss'], label='validation loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load the model\n",
    "model = tf.keras.models.load_model('mask_detector_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/andre/Documents/Image-and-Speech-Recognition-Project/.venv/lib/python3.12/site-packages/tf_keras/src/engine/training.py\", line 2436, in predict_function  *\n        return step_function(self, iterator)\n    File \"/home/andre/Documents/Image-and-Speech-Recognition-Project/.venv/lib/python3.12/site-packages/tf_keras/src/engine/training.py\", line 2421, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/andre/Documents/Image-and-Speech-Recognition-Project/.venv/lib/python3.12/site-packages/tf_keras/src/engine/training.py\", line 2409, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/home/andre/Documents/Image-and-Speech-Recognition-Project/.venv/lib/python3.12/site-packages/tf_keras/src/engine/training.py\", line 2377, in predict_step\n        return self(x, training=False)\n    File \"/home/andre/Documents/Image-and-Speech-Recognition-Project/.venv/lib/python3.12/site-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/andre/Documents/Image-and-Speech-Recognition-Project/.venv/lib/python3.12/site-packages/tf_keras/src/engine/input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_7\" is incompatible with the layer: expected shape=(None, 150, 150, 3), found shape=(None, 128, 128, 3)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m img_array \u001b[38;5;241m=\u001b[39m preprocess_image(image_path)  \u001b[38;5;66;03m# Preprocess the image\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Predict the class (probability) using the model\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m pred_prob \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_array\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Get the probability for the \"mask\" class\u001b[39;00m\n\u001b[1;32m     31\u001b[0m pred_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMask\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pred_prob \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m threshold \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo Mask\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Apply the threshold\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Store the result\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Image-and-Speech-Recognition-Project/.venv/lib/python3.12/site-packages/tf_keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filenbwenpdy.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/andre/Documents/Image-and-Speech-Recognition-Project/.venv/lib/python3.12/site-packages/tf_keras/src/engine/training.py\", line 2436, in predict_function  *\n        return step_function(self, iterator)\n    File \"/home/andre/Documents/Image-and-Speech-Recognition-Project/.venv/lib/python3.12/site-packages/tf_keras/src/engine/training.py\", line 2421, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/andre/Documents/Image-and-Speech-Recognition-Project/.venv/lib/python3.12/site-packages/tf_keras/src/engine/training.py\", line 2409, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/home/andre/Documents/Image-and-Speech-Recognition-Project/.venv/lib/python3.12/site-packages/tf_keras/src/engine/training.py\", line 2377, in predict_step\n        return self(x, training=False)\n    File \"/home/andre/Documents/Image-and-Speech-Recognition-Project/.venv/lib/python3.12/site-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/andre/Documents/Image-and-Speech-Recognition-Project/.venv/lib/python3.12/site-packages/tf_keras/src/engine/input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_7\" is incompatible with the layer: expected shape=(None, 150, 150, 3), found shape=(None, 128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Path to the test dataset\n",
    "test_dir = '../dataset/images/cropped'\n",
    "\n",
    "# Image size (same as used for training)\n",
    "image_size = (128, 128)\n",
    "\n",
    "# Threshold for binary classification\n",
    "threshold = 0.5\n",
    "\n",
    "# Function to preprocess a single image\n",
    "def preprocess_image(image_path):\n",
    "    # Load the image\n",
    "    img = load_img(image_path, target_size=image_size)  # Resize the image\n",
    "    img_array = img_to_array(img)  # Convert to NumPy array\n",
    "    img_array = img_array / 255.0  # Rescale pixel values\n",
    "    return np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "\n",
    "# Load the test dataset and make predictions\n",
    "predictions = []\n",
    "for image_name in os.listdir(test_dir):\n",
    "    if image_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "        image_path = os.path.join(test_dir, image_name)\n",
    "        img_array = preprocess_image(image_path)  # Preprocess the image\n",
    "        \n",
    "        # Predict the class (probability) using the model\n",
    "        pred_prob = model.predict(img_array)[0][0]  # Get the probability for the \"mask\" class\n",
    "        pred_class = \"Mask\" if pred_prob >= threshold else \"No Mask\"  # Apply the threshold\n",
    "        \n",
    "        # Store the result\n",
    "        predictions.append((image_name, pred_prob, pred_class))\n",
    "\n",
    "# Display results\n",
    "print(\"Predictions:\")\n",
    "for image_name, pred_prob, pred_class in predictions:\n",
    "    print(f\"Image: {image_name}, Probability: {pred_prob:.2f}, Predicted Class: {pred_class}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
